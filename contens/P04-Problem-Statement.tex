\section{Optimal Control for SHE}\label{Section4}

Since the SHE problem is equivalent to controlling a dynamic system from the origin of coordinates to a specific point, we must formulate a control problem that solves this task but also complies with the restrictions on the values that the control can take. It is necessary to set a finite subset $ \mathcal {U} $ of the interval $ [- 1,1] $, the optimal control is such that it can only take the values allowed in the discretization. In other words, the control problem is:
\newline
\begin{problem}[OCP for SHE]\label{OCP1}
    Given two sets of odd numbers $ \mathcal {E} _a $ and $ \mathcal {E} _b $ and given the target vector $ \bm {x} _T \in \mathbb {R} ^ {N_a + N_b} $, also given a set $ \mathcal {U} $ of admissible controls, we look for the function $ u (\tau) \ | \ \tau \in [0, \pi) $ such that:
    %
    \begin{gather}
        \min_{u(\tau) \in \mathcal{U}}         
         \frac{1}{2}|| \bm{x}(\pi)  ||^2   \\
        \notag \text{suject to: } \\
        \begin{cases}
            \dot{\bm{x}}(\tau) = -\big(\frac{2}{\pi}\big)\bm{\mathcal{D}}(\tau) u(\tau)  & \tau \in [0,\pi)\\
            \bm{x}(0) = \bm{x}_0
        \end{cases}
    \end{gather}
\end{problem}
%
The solution of this control problem is complex due to the restriction on the admissible control values.
%
In order to obtain a problem that can be solved by classical control theory we can formulate the following control problem:
\newline
\begin{problem}[Regularized OCP for SHE]\label{OCP2}
    Given two sets of odd numbers $ \mathcal {E} _a $ and $ \mathcal {E} _b $ and given the target vector $ \bm {x} _T \in \mathbb {R} ^ {N_a + N_b} $, we look for the function $ |u (\tau)|<1 \ | \ \tau \in [0, \pi) $ such that:

    \begin{gather}
        \min_{|u(\tau) |<1}         
         \Bigg[ \frac{1}{2}|| \bm{x}(\pi)  ||^2  
        + \epsilon \int_0^{\pi} \mathcal{L}(u(\tau)) d\tau \Bigg]  \\
        \notag \text{suject to: } \\
        \begin{cases}
            \dot{\bm{x}}(\tau) = -\big(\frac{2}{\pi}\big)\bm{\mathcal{D}}(\tau) u(\tau)  & \tau \in [0,\pi)\\
            \bm{x}(0) = \bm{x}_0
        \end{cases}
    \end{gather}
\end{problem}
Where we will choose $ \mathcal {L}: \mathbb {R} \rightarrow \mathbb {R} $ such that the optimal control $ u^* $ only takes values in the discretization $ \mathcal {U} $ of the interval $ [- 1.1] $. Furthermore, the parameter $ \epsilon $ should be small so that the solution minimizes the distance from the final state and the target.
%
Next we will study the optimality conditions of the problem, for a general $ \mathcal {L} $ function, and then specify how $ \mathcal {L} $ should be so that the optimal control $ u ^ * $ only takes the allowed values in $ \mathcal {U} $.

\subsection{Optimality conditions}

To write the optimility conditions of the problem we will use the principle of the Pontryagin minimum \cite[Chapter~2.7]{bryson1975applied}. For them it is necessary to define the Hamiltonian of the system, which in this case is:
\begin{gather}\label{hamil}
    H(u,\bm{p},\tau) = 
    \epsilon \mathcal{L}(u) -
    \bigg(\frac{2}{\pi}\bigg)[\bm{p}^T(\tau) \cdot \bm{\mathcal{D}}(\tau)]
    u(\tau)
\end{gather}
Where the variable $ \bm{p} (\tau) $ called adjoint state is introduced, which is associated with the restriction imposed by the system. Este tiene la misma dimensión del estado, de manera que 
\begin{gather}
        \bm{x}(\tau) = \begin{bmatrix}
            \bm{\alpha}(\tau) \\  \bm{\beta}(\tau)
        \end{bmatrix}   \Leftrightarrow
        \bm{p}(\tau) = \begin{bmatrix}
        \bm{p}^\alpha(\tau) \\ \bm{p}^\beta(\tau)
                \end{bmatrix}
\end{gather}
A continuación enumeraremos las condiciones de optimalidad provenientes del prinicio de mínimo de Pontryagin.

\begin{enumerate}

    \item \textbf{Final condition of the adjoint}: This optimiality condition is obtained from the cost in the final time $\tau = \pi$ of the control problem in this case $ \Psi (\bm{x}) = \frac {1}{2} \| \bm{x} (\pi) - \bm{x}_T \|^2 $.
    \begin{gather}
        \bm{p}(\pi) = \nabla_{\bm{x}} \Psi(\bm{x}) =  (\bm{x} (\pi) - \bm{x}_T)
    \end{gather}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \textbf{Adjoint evolution equation}: 
    \begin{gather}
        \dot{\bm{p}}(\tau) = -\nabla_x H(u(\tau),\bm{p}(\tau),\tau) = 0
    \end{gather}
    From where it can be deduced that $ \bm {p} (\tau)$ is a constant so that $ \bm {p} (\tau) = \bm {x} (\pi) - \bm { x} _T \ | \ \forall \tau \in [0, \pi) $ so from now on we will refer to it simply as $ \bm {p} $, noting that it is invariant in time.
    \item \textbf{Optimal  Waveform}: We known that $ u^* = \argmin_{|u|<1} H(\tau,\bm{p}^*,u)$, so in this case we can write:
    \begin{gather}
        u^*(\tau) = \argmin_{|u|<1}  \Big[   \epsilon \mathcal{L}(u(\tau)) - \Big(\frac{2}{\pi}\Big)
        [{\bm{p}^*}^T \cdot \bm{\mathcal{D}}(\tau)]
        u(\tau) \Big]
    \end{gather}
    Therefore, this optimality condition reduces to the optimization of a function in a variable within the interval $ [- 1,1] $. 
\end{enumerate}
%
Es importante recordar que 
    \begin{gather}
        [{\bm{p}^*}^T \cdot \bm{\mathcal{D}}(\tau)] = \sum_{i \in \mathcal{E}_a} p^*_\alpha \cos(i\tau) + \sum_{j \in \mathcal{E}_b} p^*_\beta \sin(j\tau) 
    \end{gather}
we build a function $\mathcal{H}_m: [-1,1] \rightarrow \mathbb{R}$ such that:
\begin{gather}
    \mathcal{H}_m(u) = \epsilon \mathcal{L}(u) - mu  |  \forall m \in \mathbb{R}
\end{gather}
Donde hemos remplazado el término $(2/\pi)[{\bm{p}^*}^T \cdot \bm{\mathcal{D}}(\tau)]$ por un parámetro $m$ que puede variar en todo la recta real. Es decir, el problema para diseñar un problema de control óptimo que solo pueda tomar valores en $\mathcal{U}$ se reduce a diseñar una función unidimensional  com parámertro $m$ cuyos mínimos sean los elementos de $\mathcal{U}$ para cualquier valor de $m$. 
\subsection{Piecewise linear penalization}

En esta subsección presentaremos como diseñar el término de penalización $\mathcal{L}(u)$ para que el control óptimo en cualquier caso este contenida en $\mathcal{U}$. 
% \begin{proposition}
%     Consideremos que tenemos un conjunto de elementos $\mathcal{U} = \{u_1,u_2,\dots,u_{N_u}\}$, además consideraremos una función de penalización $\mathcal{L}(u)$ que define la función $\mathcal{H}_m(u)$. Entonces si los puntos $\{u_k,\mathcal{L}(u_k)\}_{k=1}^{N_u}$ son la envolvente convexa de ellos mismos entonces los mínimos de $\mathcal{H}_m(u)$ son los elementos de $\mathcal{U}$.
% \end{proposition}
%
De manera más concreta podemos elegir la interpolación afín de una parábola  $\mathcal{L}:[-1,1] \rightarrow \mathbb{R}$ como término de penalización. Es decir:  
\begin{gather}\label{PLP}
    {\mathcal{L}(u)} = \begin{cases}
        \big[ (u_{k+1}+u_{k}) (u-u_k) + u_k^2 \big] & \text{if}  u \in [u_k,u_{k+1}[ \\
        1 & \text{if} \ u = u_{N_u} 
    \end{cases} \\
    \notag \forall k \in \{1,\dots,N_u-1\}
\end{gather}
%
De manera que para calcular el mínimo de $\mathcal{H}_m(u)$ deberemos considerar que esta función no es diferenciable por lo que la condicion de optimizalidad no puede realizarse con la difereciación típica sino con la subdiferencial.

Entonces calulremos la subdiferecial $\partial \mathcal{L}$ todo 
\begin{gather}
        \partial\mathcal{L}= \begin{cases}
            \{u_1 + u_2  \}   & \text{if} \ u = u_1 \\
            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
            \{u_k + u_{k+1}\}  & \text{if} \ u \in \ ]u_k,u_{k+1}[ \hspace{0.9em} \dagger\\
            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
            [u_k+u_{k-1} ,  u_{k+1}+u_k] & \text{if} \ u = u_k \hspace{3.9em} \ddagger \\
            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
            \{u_{N_u} + u_{N_u-1}  \} & \text{if} \ u = u_{N_u} 
       \end{cases} \\
       \notag \dagger \ \forall k \in \{1,\dots,N_u-1\} \hspace{1em}
       \notag \ddagger  \ \forall k \in \{2,\dots,N_u-1\}
\end{gather} 

De manera que $\partial H_m = \epsilon\partial \mathcal{L} - m$.
Entonces dado $m\in \mathbb{R}$ busamos $u \in [-1,1]$ que minimiza $\mathcal{H}_m(u)$. Es necesario comprobar que cero perteneca al subdiferecial $\partial \mathcal{H}_m(u)$

\begin{itemize}
    \item \textbf{Case 1: $m \leq \epsilon(u_1+u_2)$}: Dado que $m$ es menor que el menor de todos los subdiferenciales entonces el cero no se encuentra dentro de ninguna de los intervalos definidos para la subdiferencial. De manera que el mínimo esta en un extremo
    \begin{gather}
        \argmin_{|u| < 1} \mathcal{H}_m(u) = u_1
    \end{gather} 
    \item \textbf{Case 2: $m = \epsilon(u_{k+1}+u_k) $}:  Teniendo en cuenta que $\forall k \in \{1,\dots,N_u-1\}$.
    \begin{gather}
        \argmin_{|u| < 1} \mathcal{H}_m(u) = [u_k,u_{k+1}[ 
    \end{gather} 
    \item \textbf{Case 3: $\epsilon(u_k+u_{k-1})<m<\epsilon(u_{k+1}+u_k)$} : Teniendo en cuenta que $\forall k \in \{2,\dots,N_u-1\}$.
    \begin{gather}
        \argmin_{|u| < 1} \mathcal{H}_m(u) = u_k
    \end{gather}
    \item \textbf{Case 4: $m>\epsilon(u_{N_u}+u_{N_u-1})$}:
    \begin{gather}
        \argmin_{|u| < 1} \mathcal{H}_m(u) = u_{N_u}
    \end{gather} 
\end{itemize}

Es decir solo cuando  $m = \epsilon(u_{k+1}+u_k)$ los mínimos del Hamiltoniano esta dentro de un intervalo, para otros valores de $m$ los mínimos del Hamiltoniano están contenidos en $\mathcal{U}$. Así que ante una variación continua de $m$ el caso 2 solo ocurrirá de manera puntal. Recordando en el problema de control óptimo $m(\tau) = [\bm{p}^T(\tau) \cdot \bm{\mathcal{D}}(\tau)]$, podemos notar que el caso 2 corresponde a los instantes $\tau$ de cambio de valor.